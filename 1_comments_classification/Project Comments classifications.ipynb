{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words',quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger',quiet=True)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of two columns - the text of the comments and the indication if the comments is negative (1) or positive(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stop words variable\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "        \"\"\"\n",
    "        returns WORDNET POS in accordance with the nltk (a,n,r,v) lemmatization method \n",
    "        \"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "        if tag == 'J':\n",
    "            return wordnet.ADJ\n",
    "\n",
    "        elif tag == 'V':\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif tag == 'N':\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif tag == 'R':\n",
    "            return wordnet.ADV\n",
    "\n",
    "        else:\n",
    "            return wordnet.NOUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    returns lemmatized data \n",
    "    \"\"\"\n",
    "\n",
    "    # Leaves only letters\n",
    "\n",
    "    x = re.sub(r'[^a-zA-Z]+', ' ', x)\n",
    "\n",
    "    # Creates word tokens\n",
    "\n",
    "#     x = nltk.word_tokenize(x)\n",
    "    \n",
    "    # Returns lemmas\n",
    "    \n",
    "#     x = ' '.join([lemmer.lemmatize(w,get_wordnet_pos(w)) for w in x])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the column\n",
    "\n",
    "data['text_lemm'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "\n",
       "                                           text_lemm  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D aww He matches this background colour I m se...  \n",
       "2  Hey man I m really not trying to edit war It s...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0\n",
       "toxic        0\n",
       "text_lemm    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(['toxic','text'], axis = 1)\n",
    "target = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2646b250>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD1CAYAAAClSgmzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASu0lEQVR4nO3df6zd9X3f8edr9kiTVgQIF0ZtM7Pmtp2DNpVY4K3SVMUrmKSK+SNIRlWxUkvWMtjaaVNj1j8sJUEi2jQ2pATJCx4minAQ7YTVmLoWSRRNA8IlpBCHUt+RFN+ahpvaYWxRQp2+98f53PX0+nx8fe9x7nXw8yEdne/3/Xl/vudzJOe+8v1xL6kqJEka5e+s9AIkSecvQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2rV3oB59rll19e69evX+llSNJPlGefffa7VTUxv/6WC4n169czNTW10suQpJ8oSf5sVN3LTZKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1veV+me4nxfpdX1jpJbylfPueD6z0EqS3pAXPJJLsTfJakm+MGPt3SSrJ5W0/Se5LMp3k+STXDfVuT3K0vbYP1d+b5IU2574kafXLkhxu/YeTXHpuvrIk6WydzeWmB4Et84tJ1gG/CrwyVL4ZmGyvncD9rfcyYDdwA3A9sHvoh/79rXdu3txn7QKeqKpJ4Im2L0laRguGRFV9BTgxYuhe4HeA4f9I9lbgoRp4CrgkyVXATcDhqjpRVSeBw8CWNnZxVT1Zg//Y9kPALUPH2te29w3VJUnLZEk3rpN8EPjzqvrjeUNrgGND+zOtdqb6zIg6wJVV9SpAe79iKWuVJC3dom9cJ3kH8LvAjaOGR9RqCfXFrmkng0tWXH311YudLknqWMqZxM8B1wB/nOTbwFrga0n+HoMzgXVDvWuB4wvU146oA3ynXY6ivb/WW1BV7amqjVW1cWLitD+HLklaokWHRFW9UFVXVNX6qlrP4Af9dVX1F8AB4Pb2lNMm4PV2qegQcGOSS9sN6xuBQ23sjSSb2lNNtwOPtY86AMw9BbV9qC5JWiZn8wjsw8CTwC8kmUmy4wztB4GXgWngvwL/EqCqTgAfB55pr4+1GsBHgM+0Of8LeLzV7wF+NclRBk9R3bO4ryZJGteC9ySq6rYFxtcPbRdwR6dvL7B3RH0KuHZE/S+BzQutT5L04+Of5ZAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroWDIkke5O8luQbQ7X/kORPkjyf5L8nuWRo7K4k00leSnLTUH1Lq00n2TVUvybJ00mOJvl8kota/W1tf7qNrz9XX1qSdHbO5kziQWDLvNph4Nqq+kfAnwJ3ASTZAGwD3tPmfDrJqiSrgE8BNwMbgNtaL8AngXurahI4Cexo9R3Ayap6N3Bv65MkLaMFQ6KqvgKcmFf7o6o61XafAta27a3A/qr6YVV9C5gGrm+v6ap6uareBPYDW5MEeB/waJu/D7hl6Fj72vajwObWL0laJufinsRvAo+37TXAsaGxmVbr1d8FfG8ocObqf+tYbfz11i9JWiZjhUSS3wVOAZ+bK41oqyXUz3SsUevYmWQqydTs7OyZFy1JOmtLDokk24FfA369quZ+eM8A64ba1gLHz1D/LnBJktXz6n/rWG38ncy77DWnqvZU1caq2jgxMbHUryRJmmdJIZFkC/BR4INV9f2hoQPAtvZk0jXAJPBV4Blgsj3JdBGDm9sHWrh8CfhQm78deGzoWNvb9oeALw6FkSRpGaxeqCHJw8CvAJcnmQF2M3ia6W3A4XYv+amq+hdVdSTJI8A3GVyGuqOqftSOcydwCFgF7K2qI+0jPgrsT/IJ4DnggVZ/APhskmkGZxDbzsH3lSQtwoIhUVW3jSg/MKI21383cPeI+kHg4Ij6ywyefppf/wFw60LrkyT9+Pgb15KkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUteCIZFkb5LXknxjqHZZksNJjrb3S1s9Se5LMp3k+STXDc3Z3vqPJtk+VH9vkhfanPuS5EyfIUlaPmdzJvEgsGVebRfwRFVNAk+0fYCbgcn22gncD4Mf+MBu4AbgemD30A/9+1vv3LwtC3yGJGmZLBgSVfUV4MS88lZgX9veB9wyVH+oBp4CLklyFXATcLiqTlTVSeAwsKWNXVxVT1ZVAQ/NO9aoz5AkLZOl3pO4sqpeBWjvV7T6GuDYUN9Mq52pPjOifqbPkCQtk3N94zojarWE+uI+NNmZZCrJ1Ozs7GKnS5I6lhoS32mXimjvr7X6DLBuqG8tcHyB+toR9TN9xmmqak9VbayqjRMTE0v8SpKk+ZYaEgeAuSeUtgOPDdVvb085bQJeb5eKDgE3Jrm03bC+ETjUxt5Isqk91XT7vGON+gxJ0jJZvVBDkoeBXwEuTzLD4Cmle4BHkuwAXgFube0HgfcD08D3gQ8DVNWJJB8Hnml9H6uquZvhH2HwBNXbgcfbizN8hiRpmSwYElV1W2do84jeAu7oHGcvsHdEfQq4dkT9L0d9hiRp+fgb15KkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUtdYIZHk3yQ5kuQbSR5O8lNJrknydJKjST6f5KLW+7a2P93G1w8d565WfynJTUP1La02nWTXOGuVJC3ekkMiyRrgXwMbq+paYBWwDfgkcG9VTQIngR1tyg7gZFW9G7i39ZFkQ5v3HmAL8Okkq5KsAj4F3AxsAG5rvZKkZTLu5abVwNuTrAbeAbwKvA94tI3vA25p21vbPm18c5K0+v6q+mFVfQuYBq5vr+mqermq3gT2t15J0jJZckhU1Z8D/xF4hUE4vA48C3yvqk61thlgTdteAxxrc0+1/ncN1+fN6dUlSctknMtNlzL4f/bXAD8L/DSDS0Pz1dyUzthi66PWsjPJVJKp2dnZhZYuSTpL41xu+ufAt6pqtqr+Cvh94J8Cl7TLTwBrgeNtewZYB9DG3wmcGK7Pm9Orn6aq9lTVxqraODExMcZXkiQNGyckXgE2JXlHu7ewGfgm8CXgQ61nO/BY2z7Q9mnjX6yqavVt7emna4BJ4KvAM8Bke1rqIgY3tw+MsV5J0iKtXrhltKp6OsmjwNeAU8BzwB7gC8D+JJ9otQfalAeAzyaZZnAGsa0d50iSRxgEzCngjqr6EUCSO4FDDJ6c2ltVR5a6XknS4i05JACqajewe175ZQZPJs3v/QFwa+c4dwN3j6gfBA6Os0ZJ0tL5G9eSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVLXWCGR5JIkjyb5kyQvJvknSS5LcjjJ0fZ+aetNkvuSTCd5Psl1Q8fZ3vqPJtk+VH9vkhfanPuSZJz1SpIWZ9wzif8C/GFV/SLwj4EXgV3AE1U1CTzR9gFuBibbaydwP0CSy4DdwA3A9cDuuWBpPTuH5m0Zc72SpEVYckgkuRj4Z8ADAFX1ZlV9D9gK7Gtt+4Bb2vZW4KEaeAq4JMlVwE3A4ao6UVUngcPAljZ2cVU9WVUFPDR0LEnSMhjnTOIfALPAf0vyXJLPJPlp4MqqehWgvV/R+tcAx4bmz7TameozI+qSpGUyTkisBq4D7q+qXwL+L39zaWmUUfcTagn10w+c7EwylWRqdnb2zKuWJJ21cUJiBpipqqfb/qMMQuM77VIR7f21of51Q/PXAscXqK8dUT9NVe2pqo1VtXFiYmKMryRJGrbkkKiqvwCOJfmFVtoMfBM4AMw9obQdeKxtHwBub085bQJeb5ejDgE3Jrm03bC+ETjUxt5Isqk91XT70LEkSctg9Zjz/xXwuSQXAS8DH2YQPI8k2QG8Atzaeg8C7wemge+3XqrqRJKPA8+0vo9V1Ym2/RHgQeDtwOPtJUlaJmOFRFV9Hdg4YmjziN4C7ugcZy+wd0R9Crh2nDVKkpbO37iWJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6xg6JJKuSPJfkD9r+NUmeTnI0yeeTXNTqb2v70218/dAx7mr1l5LcNFTf0mrTSXaNu1ZJ0uKcizOJ3wJeHNr/JHBvVU0CJ4Edrb4DOFlV7wbubX0k2QBsA94DbAE+3YJnFfAp4GZgA3Bb65UkLZOxQiLJWuADwGfafoD3AY+2ln3ALW17a9unjW9u/VuB/VX1w6r6FjANXN9e01X1clW9CexvvZKkZTLumcR/Bn4H+Ou2/y7ge1V1qu3PAGva9hrgGEAbf731///6vDm9uiRpmSw5JJL8GvBaVT07XB7RWguMLbY+ai07k0wlmZqdnT3DqiVJizHOmcQvAx9M8m0Gl4Lex+DM4pIkq1vPWuB4254B1gG08XcCJ4br8+b06qepqj1VtbGqNk5MTIzxlSRJw5YcElV1V1Wtrar1DG48f7Gqfh34EvCh1rYdeKxtH2j7tPEvVlW1+rb29NM1wCTwVeAZYLI9LXVR+4wDS12vJGnxVi/csmgfBfYn+QTwHPBAqz8AfDbJNIMziG0AVXUkySPAN4FTwB1V9SOAJHcCh4BVwN6qOvJjWK8kqeOchERVfRn4ctt+mcGTSfN7fgDc2pl/N3D3iPpB4OC5WKMkafH8jWtJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKlrySGRZF2SLyV5McmRJL/V6pclOZzkaHu/tNWT5L4k00meT3Ld0LG2t/6jSbYP1d+b5IU2574kGefLSpIWZ5wziVPAv62qfwhsAu5IsgHYBTxRVZPAE20f4GZgsr12AvfDIFSA3cANwPXA7rlgaT07h+ZtGWO9kqRFWnJIVNWrVfW1tv0G8CKwBtgK7Gtt+4Bb2vZW4KEaeAq4JMlVwE3A4ao6UVUngcPAljZ2cVU9WVUFPDR0LEnSMjgn9ySSrAd+CXgauLKqXoVBkABXtLY1wLGhaTOtdqb6zIi6JGmZjB0SSX4G+D3gt6vqf5+pdUStllAftYadSaaSTM3Ozi60ZEnSWRorJJL8XQYB8bmq+v1W/k67VER7f63VZ4B1Q9PXAscXqK8dUT9NVe2pqo1VtXFiYmKcryRJGjLO000BHgBerKr/NDR0AJh7Qmk78NhQ/fb2lNMm4PV2OeoQcGOSS9sN6xuBQ23sjSSb2mfdPnQsSdIyWD3G3F8GfgN4IcnXW+3fA/cAjyTZAbwC3NrGDgLvB6aB7wMfBqiqE0k+DjzT+j5WVSfa9keAB4G3A4+3lyRpmSw5JKrqfzD6vgHA5hH9BdzROdZeYO+I+hRw7VLXKEkazzhnEpLegtbv+sJKL+Et5dv3fGCllzAW/yyHJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpK7zPiSSbEnyUpLpJLtWej2SdCE5r0MiySrgU8DNwAbgtiQbVnZVknThOK9DArgemK6ql6vqTWA/sHWF1yRJF4zVK72ABawBjg3tzwA3zG9KshPY2Xb/T5KXlmFtF4rLge+u9CIWkk+u9Aq0Avy3eW79/VHF8z0kMqJWpxWq9gB7fvzLufAkmaqqjSu9Dmk+/20uj/P9ctMMsG5ofy1wfIXWIkkXnPM9JJ4BJpNck+QiYBtwYIXXJEkXjPP6clNVnUpyJ3AIWAXsraojK7ysC42X8XS+8t/mMkjVaZf4JUkCzv/LTZKkFWRISJK6DAlJUtd5feNayyvJLzL4jfY1DH4f5ThwoKpeXNGFSVoxnkkIgCQfZfBnTwJ8lcHjxwEe9g8r6nyW5MMrvYa3Mp9uEgBJ/hR4T1X91bz6RcCRqppcmZVJZ5bklaq6eqXX8Vbl5SbN+WvgZ4E/m1e/qo1JKybJ870h4MrlXMuFxpDQnN8GnkhylL/5o4pXA+8G7lyxVUkDVwI3ASfn1QP8z+VfzoXDkBAAVfWHSX6ewZ9nX8Pgf3wzwDNV9aMVXZwEfwD8TFV9ff5Aki8v/3IuHN6TkCR1+XSTJKnLkJAkdRkSkqQuQ0KS1GVISJK6/h/Hr4Ti4CSfeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that target is not balanced, so we use the stratify parameter when dividing into test and train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features,\n",
    "                                                                            target,\n",
    "                                                                            test_size =  0.10, random_state = 42,\n",
    "                                                                            stratify = target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81604</th>\n",
       "      <td>March UTC That s some strange interpretation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78427</th>\n",
       "      <td>style background color F FFFA padding cellpad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55376</th>\n",
       "      <td>You Republic of Turkey and supporters therof a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_lemm\n",
       "81604   March UTC That s some strange interpretation ...\n",
       "78427   style background color F FFFA padding cellpad...\n",
       "55376  You Republic of Turkey and supporters therof a..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = features_train['text_lemm'].values.astype('U')\n",
    "corpus_test = features_test['text_lemm'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words = stop_words, max_features=10000)\n",
    "\n",
    "vect_train = tfidf_vectorizer.fit_transform(corpus_train)\n",
    "vect_test = tfidf_vectorizer.transform(corpus_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_features was selected experimentally, increasing it to 15-20 thousand, the metric did not change, by reducing it to less than 10 thousand, the metric worsened deteriorated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>abbreviations</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdul</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaron   ab  abandon  abandoned  abbey  abbreviation  abbreviations  \\\n",
       "0  0.0    0.0  0.0      0.0        0.0    0.0           0.0            0.0   \n",
       "1  0.0    0.0  0.0      0.0        0.0    0.0           0.0            0.0   \n",
       "2  0.0    0.0  0.0      0.0        0.0    0.0           0.0            0.0   \n",
       "3  0.0    0.0  0.0      0.0        0.0    0.0           0.0            0.0   \n",
       "4  0.0    0.0  0.0      0.0        0.0    0.0           0.0            0.0   \n",
       "\n",
       "   abc  abdul  ...  zealand  zero  zinc  zionism  zionist  zionists  zoe  \\\n",
       "0  0.0    0.0  ...      0.0   0.0   0.0      0.0      0.0       0.0  0.0   \n",
       "1  0.0    0.0  ...      0.0   0.0   0.0      0.0      0.0       0.0  0.0   \n",
       "2  0.0    0.0  ...      0.0   0.0   0.0      0.0      0.0       0.0  0.0   \n",
       "3  0.0    0.0  ...      0.0   0.0   0.0      0.0      0.0       0.0  0.0   \n",
       "4  0.0    0.0  ...      0.0   0.0   0.0      0.0      0.0       0.0  0.0   \n",
       "\n",
       "   zone  zoo  zuck  \n",
       "0   0.0  0.0   0.0  \n",
       "1   0.0  0.0   0.0  \n",
       "2   0.0  0.0   0.0  \n",
       "3   0.0  0.0   0.0  \n",
       "4   0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "f = pd.DataFrame(vect_train.toarray(), columns = feature_names)\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target is unbalanced, it's necessary to specify this in model hyperparamenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(class_weight= {0:1,1:10}, random_state= 42, max_iter = 1000,\n",
    "                              solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight={0: 1, 1: 10}, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(vect_train, target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7057026476578412\n"
     ]
    }
   ],
   "source": [
    "predict = lr_model.predict(vect_test)\n",
    "print(f1_score(target_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[13416   919]\n",
      " [  237  1386]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix')\n",
    "print(confusion_matrix(target_test,predict))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = LGBMClassifier(random_state=42, class_weight= {0:1,1:8},learning_rate = 0.2,\n",
    "                           n_estimators = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_distributions = dict(max_depth = [15,30], max_bin=[100,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_rand_clf =RandomizedSearchCV(lgb_model, param_distributions =light_distributions ,scoring='f1', n_jobs= -1,\n",
    "                   cv=3,random_state= 42, n_iter = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 15, 'max_bin': 300}\n",
      "0.7607984883997331\n"
     ]
    }
   ],
   "source": [
    "light_rand_clf.fit(vect_train, target_train)\n",
    "print(light_rand_clf.best_params_)\n",
    "print(light_rand_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = LGBMClassifier(random_state=42, class_weight= {0:1,1:8},learning_rate = 0.5,\n",
    "                           n_estimators = 300, max_depth = 15, max_bin = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751338488994646\n"
     ]
    }
   ],
   "source": [
    "lgb_model.fit(vect_train, target_train)\n",
    "predictions_lgb = lgb_model.predict(vect_test)\n",
    "print(f1_score(target_test,predictions_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[13859   476]\n",
      " [  360  1263]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix')\n",
    "print(confusion_matrix(target_test,predictions_lgb))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many attempts to process the text and improve the metrics, I came to the conclusion that lemmatisation did not play a significant role in processing comments. Perhaps when processing plain text, where all words are used in the correct form, lemmarisation would have played a greater role. In toxic comments there are many extra letters used, words are misspelled : it is more important to use these words as they are. Lemmatisation by part of speech takes a very long time, so if it is possible not to use it, I would do that.\n",
    "\n",
    "In the same way, attempts to limit the vectorizer have not been successful: by limiting the upper or lower boundary of the most used words, we exclude words that have proven important to the model to determine whether or not they are toxic comments.   \n",
    "Limiting the number of features had the following effect: allocating less than 10,000 worsened the metric and  increased the model's running time. \n",
    "\n",
    "Two models were tested: logistic regression and LightGBM. The best result was achieved by the LightGBM model: F1 score 0.75. LightGBM model has found slightly less toxic comments, but it was two times less false positive comments than Logistic regression. It seems to me that the online shop also needs to decide what is more important: is it worth catching all toxic comments at expense of having too much false positive comments or not. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
